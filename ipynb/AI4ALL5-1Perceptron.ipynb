{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fear of the LORD is the beginning of knowledge, but fools despise wisdom and discipline. Proverbs 1:7\n",
    "\n",
    "-------\n",
    "\n",
    "# Welcome to \"AI for All\"\n",
    "\n",
    "Lecture Notes by idebtor@gmail.com, Handong Global University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 5. 이진 분류(Binary Classficiation)\n",
    "\n",
    "선형 회귀를 확장하여 분류(Classification)모델을 만들어 봅니다. 분류 모델은 데이터를 분류하는 방법을 학스비다. 예를 들면, 암종양을 분류하는 모델은 \"암 종양인지 아닌지'를 분류합니다. 이런 모델은 특별히 이진 분류라고 부릅니다. 이진 분류를 위한 로지스틱 회귀(logistic regression)에 대해서도 알아 봅니다. 로지스틱 회귀를 이해하고 나면, 여러분은 기계학습과 딥러닝에 한층 더 익숙해져 있을 것입니다. \n",
    "\n",
    "    5.1 퍼셉트론\n",
    "    5.2 시그모이드 함수\n",
    "    5.3 로지스틱 손실함수를 경사하강법에 적용하기\n",
    "    5.4 이진 분류를 위한 데이터셋 준비\n",
    "    5.5 로지스틱 회귀 뉴론 만들기\n",
    "    5.6 로지스틱 회귀 뉴론의 단층 신경망 만들기\n",
    "    5.7 사이킷런으로 로지스틱 회귀 수행하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 퍼셉트론(Perceptron)\n",
    "\n",
    "인공 신경망을 구성하는 기본 단위는 인공 뉴론입니다. 여러 인공 뉴론들을 체계적으로 연결하여 인공 신경망을 구성합니다. 인공 신경망의 각 인공 뉴론을 전산 과학에서는 노드$^{node}$, 유닛$^{unit}$ 혹은 간단히 뉴론이라고 하며, __퍼셉트론__이라 부르기도 합니다. 퍼셉트론은 인공 신경망의 한 종류로 인식되며, 1957년에 코넬 항공 연구소의 프랑크 로젠블라트$^{Frank \\ Rosenblatt}$가 고안한 알고리즘입니다. 그가 발표한 논문(The perceptron, a perceiving and recognizing automaton Project Para.)에서 하나의 뉴론이 정보를 받은 후, 그 정보를 다음 뉴론으로 전달할 것인가 전달하지 않을 것인가를 결정하기 위하여, 자기가 받은 입력 특성$^{features}$에 곱하는 최적의 가중치$^{weights}$를 자동적으로 학습하는 퍼셉트론 알고리즘을 제안하였습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/Rosenblatt.png?raw=true\" width=200></img>\n",
    "<center> 그림 1: 프랑크 로젠블라트 </center>\n",
    "<center> (출처 : https://commons.wikimedia.org/wiki/File:Rosenblatt_21.jpg)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 앞 강의에서 하나의 입력 값과 하나의 가중치를 가지고 있었지만, 여기서는 입력 신호와 그에 가중치가 하나 혹은 그 이상 많아지고, 마지막 단계에서 샘플을 이진 분류하기 위하여 계단 함수(step function)을 사용합니다.\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-neuron3.png?raw=true\" width=500></img>\n",
    "<center> 그림 2: 간단한 페셉트론의 구조 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 우리가 다룬 뉴론에서는 입력 $x$가 하나였지만, 이제는 둘, $x_1, x_2$로 늘어났고, 따라서 가중치도 같은 수로 증가 되었습니다. 뉴론은 입력 신호를 받아 z를 만듭니다. 즉, 다음 수식에 의해 z를 만들고, 우리는 이 수식을 __'선형 함수'__ 라고 부르겠습니다. \n",
    "\n",
    "\\begin{align}\n",
    "z = w_1x_1 + w_2x_2 + b  \\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "계단 함수는 $z$가 0보다 크거나 같으면 1로 0보다 작으면 -1로 분류합니다. 즉, 다음 연산을 진행합니다. \n",
    "\n",
    "\\begin{align} \n",
    "  y &= \n",
    "  \\begin{cases}\n",
    "   \\ 1 & \\text{$if \\ z \\ge 0$} \\\\\n",
    "   \\ -1 & \\text{$otherwise$} \\\\ \\tag{2}\n",
    "  \\end{cases}\n",
    "\\end{align}  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 1을 양성 클래스(postive class) -1을 음성 클래스(negative class)라고 부르면 위의 함수를 그래프로 그리면 다음과 같이 계단 모양이 됩니다. 그래서 계단 함수라고 부르게 된 것입니다. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-neuron4.png?raw=true\" width=300></img>\n",
    "<center> 그림 3: 계단 함수(step function) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 퍼셉트론이 어떤 구조로 되어 있는 알게 되었습니다. 쉽게 말해 퍼셉트론은 선형 함수를 통과한 값 $z$를 계단 함수로 보내 0보다 큰지 작은지 검사하여 1과 -1로 분류하는 아주 간단한 알고리즘입니다. 퍼셉트론은 계단 함수의 결과를 사용하 가중치와 편향을 조정합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그럼 이제부터 여러 개의 특성을 사용하여 문제를 해결하는 경우가 많이 나오므로,다음과 같은 퍼셉트론에도 익숙해져야 합니다.\n",
    "\n",
    "\\begin{align}\n",
    "z = w_1x_1 + w_2x_2 + b  \\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "위의 식에서 $w_1x_1$은 첫번째 특성의 가중치와 입력입니다. 이를 응용하면 특성이 $n$개인 선형함수를 다음과 같이 표기할 수 있습니다. \n",
    "\n",
    "\\begin{align}\n",
    "z = w_1x_1 + w_2x_2 + ... + w_nx_n + b  \\tag{3}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나, 연구자들은 이렇게 장황하게 늘어놓은 식을 좋아하지 않습니다. 매번 쓰기도 번거롭죠. 그래서 덧셈을 축약해서 표현하는 시그마($\\sum$)기호를 사용하여 위 식을 다음과 같이 표기합니다. \n",
    "\n",
    "\\begin{align}\n",
    "z = b + \\sum_{i=1}^{n} w_ix_i  \\tag{4}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 상수 항(b)은 시그마 기호 뒤가 아니라 앞에 두는 것이 좋습니다. 상수 항을 시그마 기호 뒤에 두면 시그마 기호에 상수 항이 포함되었다고 착각할 수 있기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 그림은 여러 특성을 가진 퍼셉트론이 다수의  입력$^{input}$을 받아서 하나의 결과를 출력$^{output}$합니다. 그의 구성을 좀 더 자세히 설명한 그림입니다. 이러한 퍼셉트론 알고리즘은 사이킷런 패키지에서 Perceptron이란 이름의 클래스를 제공합니다.  \n",
    "\n",
    "좀 더 상세한 내용을 페셉트로의 구현은 K-MOOC \"파이썬으로 배우는 기계학습\"을 참고하길 바랍니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/Perceptron-process.png?raw=true\" width=800></img>\n",
    "<center>그림 4: 퍼셉트론 알고리즘의 전체 과정</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 아달린(Adaline)\n",
    "\n",
    "퍼셉트론이 등장한 이후, 1960년에 스탠포드 대학의 버나드 위드로우(Bernard Widrow)와 테드 호프(Tedd Hoff)가 퍼셉트론을 개선한 적응형 선형 뉴런(Adaptive Linear Neuron)을 발표하였습니다. 적응형 선령 뉴론은 아달린(Adaline)이라고도 붋니다. 아달린은 선형함수의 결과를 __학습에 사용합니다__. 계단 함수의 결과는 예측에만 활용하구요. (퍼셉트론은 계단 함수의 결과를 학습에 사용합니다)\n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-neuron6.png?raw=true\" width=500></img>\n",
    "<center> 그림 5: 아달린의 구조 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "역방향 계산이 계단 함수 출력 이후에 일어나지 않고, 선형 함수 출력 직후에 진행되는 점에 주목하세요.  이 장에서 본격적으로 공부하게 될 로지스틱 회귀는 아달린의 개선 버전이므로 이 구조를 유심히 살펴보기 바랍니다. 아달린의 나머지 요소는 퍼셉트론과 동일합니다. \n",
    "\n",
    "다음 그림은 아달린의 구체적인 게산 방법을 보여주는데, 이에 대한 좀 더 상세한 설명과 구현은 K-MOOC \"파이썬으로 배우는 기계학습\"을 참고하길 바랍니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/adalineAlgorithm.png?raw=true\" width=\"500\">\n",
    "<center>그림 6: 아달린 알고리즘의 상세한 개념도</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 로지스틱 회귀(Logistic Regression)\n",
    "\n",
    "로지스틱 회귀는 \"회귀\" Regression알고리즘이 아니라 사실상 분류 알고리즘인 것에 유의하십시오. 로지스틱 회귀는 아달린에서 조금 더 발전한 형태입니다. 다음 그림을 먼저 살펴보십시오. \n",
    "\n",
    "<img src=\"https://github.com/idebtor/KMOOC-ML/blob/master/ipynb/images/ai4all-neuron7.png?raw=true\" width=600></img>\n",
    "<center> 그림 7: 로지스틱 회귀의 구조 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로지스틱 회귀는 선형 함수를 통과시켜 얻은 $z$를 임계함수(계단함수)에 보내기 전에 변형시키는데, 바로 이런 함수를 활성화 함수(activation function)라고 부릅니다.  활성화 함수를 통과한 값이 $a$로 표현되어 있는데, 앞으로 $a$라고 하면 활성화 함수를 통과한 값이라고 이해하면 됩니다. 로지스틱 회귀는 마지막 단계에서 임계 함수(threshold function)을 사용하여 예측을 수행합니다.  임계함수는 아달린이나 퍼셉트론의 계단함수와 역할은 비슷하지만 활성화 함수의 출력값을 사용한다는 점이 다릅니다.  지금은 로지스틱 회귀의 전체 구조만 살펴본 것입니다. 다음 실습을 진행하면서, 좀 더 깊이 살펴볼 것입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활성화 함수는 비선형 함수를 사용합니다. \n",
    "\n",
    "활성화 함수로는 보통 비선형 함수를 사용합니다. 다음은 비선형 함수의 한 예입니다. \n",
    "\n",
    "\\begin{align}\n",
    "p = \\frac{1}{1 + e^{-z}} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왜 활성화 함수는 비선형 함수를 사용할까요? \n",
    "\n",
    "만약 활성화 함수가 선형함수라면 어떻게 될까요? 예를 들면, 선형 함수 $ a = w_1x_1 + w_2x_2 + ... + w_nx_n$과 활성화 함수 $y = ka$가 있다고 합시다. 그러면, 이 둘은 쌓은 수식은 다음과 같다고 할 수 있습니다. \n",
    "\n",
    "\\begin{align}\n",
    "y = k(w_1x_1 + w_2x_2 + ... + w_nx_n) \n",
    "\\end{align}\n",
    "\n",
    "두 식을 덧셈솨 곱셈의 결합법칙과 분배법칙에 의하여 정리하면 다시 하나의 큰 선형 함수가 됩니다. 이렇게 되면 임계 함수 앞에 뉴론을 여거 개 쌓아도 결국 선형 함수일 것이므로 별 의미가 없습니다. 그래서, 활성화 함수는 의무적으로 비선형 함수를 사용합니다. 그럼, 로지스틱 회귀에는 어떤 활성화 함수가 사용되었을까요? 로지스틱 최귀의 활성화 함수는 바로 '시그모이드(sigmoid)'함수 입니다. 이것을 곧 살펴보겠습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고문헌\n",
    "\n",
    "1. 케라스 창시자에게 배우는 딥러닝, 프랑소와 숄레, 길벗\n",
    "1. 핸즈온 머신러닝, 오렐리앙 제롱, 한빛미디어\n",
    "1. 딥러닝 입문, 박해선, 이지스 퍼블리싱\n",
    "1. 파이썬으로 배우는 기계학습, 김영섭, K-MOOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "__Be joyful always!__ 1 Thes.5:16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
